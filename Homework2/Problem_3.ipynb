{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Training an image classifier\n",
    "----------------------------\n",
    "We will do the following steps in order:\n",
    "1. Load and normalizing the MNIST training and test datasets using\n",
    "   ``torchvision``\n",
    "2. Define a SVM model\n",
    "3. Define a loss function\n",
    "4. Train the model on the training data\n",
    "5. Test the model on the test data\n",
    "1. Loading and normalizing MNIST\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "Using ``torchvision``, itâ€™s extremely easy to load MNIST.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import itertools\n",
    "\n",
    "########################################################################\n",
    "# The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "# We transform them to Tensors of normalized range [-1, 1].\n",
    "# .. note::\n",
    "#     If running on Windows and you get a BrokenPipeError, try setting\n",
    "#     the num_worker of torch.utils.data.DataLoader() to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2     3     8     6\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Let us show some of the training images, for fun.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "examples = enumerate(trainloader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(example_data[:4]))\n",
    "\n",
    "\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[example_targets[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# 2. Define a SVM model\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.svm_layer = nn.Linear(784, 10) ### YOUR CODE ###, ### YOUR CODE ###\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # shape of input (=x): [16, 1, 28, 28]\n",
    "        # shape of output: [16, 10]\n",
    "        x = x.view(-1, 1 * 28 * 28)\n",
    "        prediction = self.svm_layer(x) ### YOUR CODE ####\n",
    "        return prediction\n",
    "                                \n",
    "model = Model().to(pytorch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# 3. Define a Loss function and optimizer\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(pytorch_device) ### YOUR CODE ###\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1 ) ### YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# 4. Train the model\n",
    "# ^^^^^^^^^^^^^^^^^^^^\n",
    "#\n",
    "# This is when things start to get interesting.\n",
    "# We simply have to loop over our data iterator, and feed the inputs to the\n",
    "# model and optimize.\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(pytorch_device), labels.to(pytorch_device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels) ### YOUR CODE ###\n",
    "        loss.backward() ### YOUR CODE ###\n",
    "        optimizer.step() ### YOUR CODE ###\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# See `here <https://pytorch.org/docs/stable/notes/serialization.html>`_\n",
    "# for more details on saving PyTorch models.\n",
    "#\n",
    "# 5. Test the model on the test data\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#\n",
    "# We have trained the model for 2 passes over the training dataset.\n",
    "# But we need to check if the model has learnt anything at all.\n",
    "#\n",
    "# We will check this by predicting the class label that the model\n",
    "# outputs, and checking it against the ground-truth. If the prediction is\n",
    "# correct, we add the sample to the list of correct predictions.\n",
    "#\n",
    "# Okay, first step. Let us display an image from the test set to get familiar.\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "images, labels = images.to(pytorch_device), labels.to(pytorch_device)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "########################################################################\n",
    "# Okay, now let us see what the model thinks these examples above are:\n",
    "\n",
    "outputs = model(images)\n",
    "\n",
    "########################################################################\n",
    "# The outputs are energies for the 10 classes.\n",
    "# The higher the energy for a class, the more the model\n",
    "# thinks that the image is of the particular class.\n",
    "# So, let's get the index of the highest energy:\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "\n",
    "########################################################################\n",
    "# The results seem pretty good.\n",
    "#\n",
    "# Let us look at how the model performs on the whole dataset.\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(pytorch_device), labels.to(pytorch_device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the model on the 60000 train images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "cmt = torch.zeros(10,10, dtype=torch.int64)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(pytorch_device), labels.to(pytorch_device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            cmt[labels[i], predicted[i]] += 1\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "print('Accuracy of the model on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cmt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-740a46b7264d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cmt' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plot_confusion_matrix(cmt.numpy(), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
